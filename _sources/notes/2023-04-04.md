---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.14.1
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# ML Task Review and Cross Validation



## Relationship between Tasks

We learned classification first, because it shares similarities with each
regression and clustering, while regression and clustering have less in common.

Classification is supervised learning for a categorical target.  
Regression is supervised learning for a continuous target.
Clustering is unsupervised learning for a categorical target.


Sklearn provides a nice flow chart for thinking through this.  

![estimator flow chart](https://scikit-learn.org/stable/_static/ml_map.png)

Predicting a category is another way of saying categorical target. Predicting a
quantitiy is another way of saying continuous target. Having lables or not is
the difference between


The flowchart assumes you know what you want to do with data and that is the
ideal scenario. You have a dataset and you have a goal.
For the purpose of getting to practice with a variety of things, in this course
we ask you to start with a task and then find a dataset. Assignment 9 is the
last time that's true however. Starting with Assignment 10 and the last
portflios, you can choose and focus on a specific application domain and then
choose the right task from there.  

Thinking about this, however, you use this information to move between the tasks
within a given type of data.
For example, you can use the same data for clustering as you did for classification.
Switching the task changes the questions though: classification evaluation tells
us how separable the classes are given that classifiers decision rule. Clustering
can find other subgroups or the same ones, so the evaluation we choose allows us
to explore this in more ways.

Regression requires a continuous target, so we need a dataset to be suitable for
that, we can't transform from the classification dataset to a regression one.  
However, we can go the other way and that's how some classification datasets are
created.

The UCI [adult](https://archive.ics.uci.edu/ml/datasets/adult) Dataset is a popular ML dataset that was dervied from census
data. The goal is to use a variety of features to predict if a person makes
more than $50k per year or not. While income is a continuous value, they applied
a threshold ($50k) to it to make a binary variable. The dataset does not include
income in dollars, only the binary indicator.  


```{admonition} Further Reading
Recent work reconsturcted the dataset with the continuous valued income.
Their [repository](https://github.com/zykls/folktables) contains the data as well
as links to their paper and a video of their talk on it.
```


## Cross Validation

This week our goal is to learn how to optmize models. The first step in that is
to get a good estimate of its performance.  

We have seen that the test train splits, which are random, influence the
performance.

```{code-cell} ipython3
# basic libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# models classes
from sklearn import tree
from sklearn import cluster
from sklearn import svm
# datasets
from sklearn import datasets
# model selection tools
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn import metrics
```

We'll use the Iris data with a decision tree.
```{code-cell} ipython3
iris_df = sns.load_dataset('iris')

iris_X = iris_df.drop(columns=['species'])
iris_y = iris_df['species']
```

```{code-cell} ipython3
dt =tree.DecisionTreeClassifier()
```

We can split the data, fit the model, then compute a score, but since the
splitting is a randomized step, the score is a random variable.


```{code-cell} ipython3
iris_X_train, iris_X_test, iris_y_train, iris_y_test = train_test_split(iris_X,iris_y)
dt.fit(iris_X_train,iris_y_train)
dt.score(iris_X_test,iris_y_test)
```

Since it is random, if we repeat this, we will generally get a different value
```{code-cell} ipython3
iris_X_train, iris_X_test, iris_y_train, iris_y_test = train_test_split(iris_X,iris_y)
dt.fit(iris_X_train,iris_y_train)
dt.score(iris_X_test,iris_y_test)
```


For example, if we have a coin that we want to see if it's fair or not. We would
flip it to test.  One flip doesn't tell us, but if we flip it a few times, we
can estimate the probability it is heads by counting how many of the flips are
heads and dividing by how many flips.  



We can do something similar with our model performance. We can split the data
a bunch of times and compute the score each time.

`cross_val_score` does this all for us.

It takes an estimator object and the data.

By default it uses 5-fold cross validation. It splits the data into 5 sections,
then uses 4 of them to train and one to test. It then iterates through so that
each section gets used for testing.

```{code-cell} ipython3
cross_val_score(dt, iris_X_train,iris_y_train)
```
We will still use the test train split to keep our test data separate from the data that we use to find our preferred parameters.

We get back a score for each section or "fold" of the data. We can average those
to get a single estimate.

```{code-cell} ipython3
cross_val_score(dt, iris_X_train,iris_y_train).mean()
```


We can change it to 10-fold.
```{code-cell} ipython3
cross_val_score(dt, iris_X_train,iris_y_train,cv=10)
```

```{code-cell} ipython3
cross_val_score(dt, iris_X_train,iris_y_train,cv=10).mean()
```


## What Does Cross validation really do?

```{important}
This is extra detail that was not presented in class.
```

It uses StratifiedKfold for classification, but since we're using regression it will use `KFold`. `test_train_split` uses `ShuffleSplit` by default, let's load that too to see what it does.

```{warning}
The key in the following is to get the _concepts_ not all of the details in how I evaluate and visualize.  I could have made figures separately to explain the concept, but I like to show that Python is self contained.
```


```{code-cell} ipython3
from sklearn.model_selection import KFold, ShuffleSplit
```

```{code-cell} ipython3
kf = KFold(n_splits = 10)
```

When we use the `split` method it gives us a generator.


```{code-cell} ipython3
kf.split(diabetes_X, diabetes_y)
```

We can use this in a loop to get the list of indices that will be used to get the test and train data for each fold.  To visualize what this is  doing, see below.

```{code-cell} ipython3
N_samples = len(diabetes_y)
kf_tt_df = pd.DataFrame(index=list(range(N_samples)))
i = 1
for train_idx, test_idx in kf.split(diabetes_X, diabetes_y):
    kf_tt_df['split ' + str(i)] = ['unused']*N_samples
    kf_tt_df['split ' + str(i)][train_idx] = 'Train'
    kf_tt_df['split ' + str(i)][test_idx] = 'Test'
    i +=1
```

```{margin}
How would you use those indices to get a out actual test and train data?
```

We can count how many times 'Test' and 'Train' appear
```{code-cell} ipython3
count_test = lambda part: len([v for v in part if v=='Test'])
count_train = lambda part: len([v for v in part if v=='Train'])
```

When we apply this along `axis=1` we to check that each sample is used in exactly 1 test set how may times each sample is used
```{code-cell} ipython3
sum(kf_tt_df.apply(count_test,axis = 1) ==1)
```


and exactly 9 training sets
```{code-cell} ipython3
sum(kf_tt_df.apply(count_test,axis = 1) ==9)
```

the describe helps ensure that all fo the values are exa

We can also visualize:
````{margin}
```{tip}
`sns.heatmap` doesn't work on strings, so we can replace them for the plotting
```
````

```{code-cell} ipython3
cmap = sns.color_palette("tab10",10)
g = sns.heatmap(kf_tt_df.replace({'Test':1,'Train':0}),cmap=cmap[7:9],cbar_kws={'ticks':[.25,.75]},linewidths=0,
    linecolor='gray')
colorbar = g.collections[0].colorbar
colorbar.set_ticklabels(['Train','Test'])
```
Note that unlike [`test_train_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) this does not always randomize and shuffle the data before splitting.

 If we apply those `lambda` functions along `axis=0`, we can see the size of each test set

```{code-cell} ipython3
kf_tt_df.apply(count_test,axis = 0)
```

and training set:
```{code-cell} ipython3
kf_tt_df.apply(count_train,axis = 0)
```

We can verify that these splits are the same size as what `test_train_split` does using the right settings.  10-fold splits the data into 10 parts and tests on 1, so that makes a test size of 1/10=.1, so we can use the `train_test_split` and check the length.

```
X_train2,X_test2, y_train2,y_test2 = train_test_split(diabetes_X, diabetes_y ,
                                                  test_size=.1,random_state=0)

[len(split) for split in [X_train2,X_test2,]]
```

Under the hood `train_test_split` uses `ShuffleSplit`
We can do a similar experiment as above to see what `ShuffleSplit` does.

```{code-cell} ipython3
skf = ShuffleSplit(10)
N_samples = len(diabetes_y)
ss_tt_df = pd.DataFrame(index=list(range(N_samples)))
i = 1
for train_idx, test_idx in skf.split(diabetes_X, diabetes_y):
    ss_tt_df['split ' + str(i)] = ['unused']*N_samples
    ss_tt_df['split ' + str(i)][train_idx] = 'Train'
    ss_tt_df['split ' + str(i)][test_idx] = 'Test'
    i +=1

ss_tt_df
```


And plot

```{code-cell} ipython3
cmap = sns.color_palette("tab10",10)
g = sns.heatmap(ss_tt_df.replace({'Test':1,'Train':0}),cmap=cmap[7:9],cbar_kws={'ticks':[.25,.75]},linewidths=0,
    linecolor='gray')
colorbar = g.collections[0].colorbar
colorbar.set_ticklabels(['Train','Test'])
```


## Cross validation with clustering
We can use *any* estimator object here.


```{code-cell} ipython3
km = cluster.KMeans(n_clusters=3)
```

```{code-cell} ipython3
cross_val_score(km, iris_X_train,)
```

```{code-cell} ipython3
km.score()
```



## Grid Search Optimization

We can optimize, however to determing the different parameter settings.

A simple way to do this is to fit the model for different parameters and score for each and compare.


```{code-cell} ipython3
param_grid = {'n_clusters':[2,3,4,5,6]}
km_opt = GridSearchCV(km, param_grid,metrics.silhouette_score)
```


The `GridSearchCV` object is constructed first and requires an estimator object and a dictionary that describes the parameter grid to search over.
The dictionary has the parameter names as the keys and the values are the values for that parameter to test.

The `fit` method on the Grid Search object fits all of the separate models.

In this case we optimize of a one dimensional "grid" just a set of values for one parameter, the number of clusters. 

```{code-cell} ipython3
param_grid = {'n_clusters':[2,3,4,5,6]}
km_opt = GridSearchCV(km, param_grid)
```

```{code-cell} ipython3
iris_X_train.shape
```

```{code-cell} ipython3
km_opt.fit(iris_X_train)
```
```{important}
I still need to explore this question. A volunteer who wants to do this for a portfolio section can do that as well
```
Why does `,scoring=metrics.silhouette_score` not work?

```{code-cell} ipython3
km_opt.best_params_
```

```{code-cell} ipython3
type(km_opt.best_estimator_)
```

We note that this is a dictionary, so to make it more readable, we can make it a DataFrame.
```{code-cell} ipython3
pd.DataFrame(km_opt.cv_results_)
```

## Optimizing a Decision Tree


Today we will optimize a decision tree over three parameters. One is the criterion, which is how it decides where to create thresholds in parameters. Gini is the default and it computes how concentrated each class is at that node, another is entropy, entropy is, generally how random something is.  Intuitively these do similar things, which makes sense because they are two ways to make the same choice, but they have slightly different calculations.

The other two parameters we have seen some before. Max depth is the height of the tree and min smaples per leaf makes it keeps the leaf sizes small.


```{code-cell} ipython3
dt = tree.DecisionTreeClassifier()
params_dt = {'criterion':['gini','entropy'],'max_depth':[2,3,4],
             'min_samples_leaf':list(range(2,20,2))}
```

what parameters give the highest accuracy? and is the most acurate one also the fastest one?


```{code-cell} ipython3
dt_opt = GridSearchCV(dt,params_dt)
dt_opt.fit(iris_X_train,iris_y_train)
```

We will fit it with default CV settings.
And we can see the best parameters

```{code-cell} ipython3
dt_opt.best_params_
```

and we can use ti to get predictions

```{code-cell} ipython3
y_pred = dt_opt.predict(iris_X_test)
```


```{code-cell} ipython3
dt_df = pd.DataFrame(dt_opt.cv_results_)
dt_df.shape
```

```{code-cell} ipython3
dt_df.columns
```

```{code-cell} ipython3
dt_df['mean_score_time'].idxmin() == dt_df['mean_test_score'].idxmax()
```

```{code-cell} ipython3
dt_df['mean_test_score'].idxmax(), dt_df['mean_score_time'].idxmin()
```


```{important}
Remember that best is context dependent and relative.  The best accuracy might not be the best overall. Automatic optimization can only find the best thing in terms of a single score.
```

```{code-cell} ipython3

```
