{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e70c1db",
   "metadata": {},
   "source": [
    "# What is ML? \n",
    "\n",
    "![ML flow](https://rhodyprog4ds.github.io/BrownSpring23/_images/MLdataflow.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ccfe089",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics \u001b[38;5;28;01mas\u001b[39;00m skmetrics\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maif360\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics \u001b[38;5;28;01mas\u001b[39;00m fairmetrics\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maif360\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BinaryLabelDataset\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics as skmetrics\n",
    "from aif360 import metrics as fairmetrics\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "import seaborn as sns\n",
    "\n",
    "compas_clean_url = 'https://raw.githubusercontent.com/ml4sts/outreach-compas/main/data/compas_c.csv'\n",
    "compas_df = pd.read_csv(compas_clean_url,index_col = 'id')\n",
    "\n",
    "compas_df = pd.get_dummies(compas_df,columns=['score_text'],)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d92fb0a",
   "metadata": {},
   "source": [
    "We may get a warning which is **okay**. If you run the cell again it will go away."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c1a2b5",
   "metadata": {},
   "source": [
    "## The COMPAS data\n",
    "\n",
    "We are going to continue with the ProPublica COMPAS audit data.  Remember it contains: \n",
    "* `age`: defendant's age\n",
    "* `c_charge_degree`: degree charged (Misdemeanor of Felony)\n",
    "* `race`: defendant's race\n",
    "* `age_cat`: defendant's age quantized in \"less than 25\", \"25-45\", or \"over 45\"\n",
    "* `score_text`: COMPAS score: 'low'(1 to 5), 'medium' (5 to 7), and 'high' (8 to 10).\n",
    "* `sex`: defendant's gender\n",
    "* `priors_count`: number of prior charges\n",
    "* `days_b_screening_arrest`: number of days between charge date and arrest where defendant was screened for compas score\n",
    "* `decile_score`: COMPAS score from 1 to 10 (low risk to high risk)\n",
    "* `is_recid`: if the defendant recidivized\n",
    "* `two_year_recid`: if the defendant within two years\n",
    "* `c_jail_in`: date defendant was imprisoned\n",
    "* `c_jail_out`: date defendant was released from jail\n",
    "* `length_of_stay`: length of jail stay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7e2f12",
   "metadata": {},
   "source": [
    "First, we will look at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ca1b52b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_df\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_df' is not defined"
     ]
    }
   ],
   "source": [
    "compas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2fe409",
   "metadata": {},
   "source": [
    "Notice the last three columns.  When we use `pd.getdummies` with its `columns` parameter, then we can append the columns all at once and they get the original column name prepended to the value in the new column name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b9460a",
   "metadata": {},
   "source": [
    "We use the `two_year_recid` as the basis of our audit because it is the real outcome that the designers of COMPAS were hoping to predict.  Sicne the COMPAS score is on a scale of 1-10, we transform to a binary variable by thresholding it (eg all above t are 1, below are 0).  We use the `score_text` instead of `decile_score` in our thresholding so that we use a recommended threshold. \n",
    "\n",
    "\n",
    "More common is to use medium or high to check accuracy (or not low) we can calulate tihs by either summing two or inverting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1996a47d",
   "metadata": {},
   "source": [
    "let's do it by inverting here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27ecf345",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m int_not \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m a:\u001b[38;5;28mint\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m(a))\n\u001b[0;32m----> 2\u001b[0m compas_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_text_MedHigh\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcompas_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_text_Low\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(int_not)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_df' is not defined"
     ]
    }
   ],
   "source": [
    "int_not = lambda a:int(not(a))\n",
    "compas_df['score_text_MedHigh'] = compas_df['score_text_Low'].apply(int_not)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513be5cd",
   "metadata": {},
   "source": [
    "Let's review computing the accruacy with sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed2ff53a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'skmetrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mskmetrics\u001b[49m\u001b[38;5;241m.\u001b[39maccuracy_score(compas_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo_year_recid\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      2\u001b[0m                          compas_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_text_High\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'skmetrics' is not defined"
     ]
    }
   ],
   "source": [
    "skmetrics.accuracy_score(compas_df['two_year_recid'],\n",
    "                         compas_df['score_text_High'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26f44c83",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'skmetrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mskmetrics\u001b[49m\u001b[38;5;241m.\u001b[39maccuracy_score(compas_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo_year_recid\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      2\u001b[0m                          compas_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_text_MedHigh\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'skmetrics' is not defined"
     ]
    }
   ],
   "source": [
    "skmetrics.accuracy_score(compas_df['two_year_recid'],\n",
    "                         compas_df['score_text_MedHigh'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780ea3ec",
   "metadata": {},
   "source": [
    "## What about breaking it down by race?\n",
    "\n",
    "Recall, we used groupby to get the per race score by creating a `lambda` function that we could apply to the groupby object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0acc439b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m compas_race \u001b[38;5;241m=\u001b[39m \u001b[43mcompas_df\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_df' is not defined"
     ]
    }
   ],
   "source": [
    "compas_race = compas_df.groupby('race')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d060b2",
   "metadata": {},
   "source": [
    "We can apply our method to each part of the  groupby object with `apply`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf75a0a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_race' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m acc_fx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m d: skmetrics\u001b[38;5;241m.\u001b[39maccuracy_score(d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo_year_recid\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      2\u001b[0m                          d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_text_MedHigh\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 4\u001b[0m \u001b[43mcompas_race\u001b[49m\u001b[38;5;241m.\u001b[39mapply(acc_fx,)\u001b[38;5;241m.\u001b[39mreset_index()\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m0\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_race' is not defined"
     ]
    }
   ],
   "source": [
    "acc_fx = lambda d: skmetrics.accuracy_score(d['two_year_recid'],\n",
    "                         d['score_text_MedHigh'])\n",
    "\n",
    "compas_race.apply(acc_fx,).reset_index().rename(columns={0:'accuracy'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4779a555",
   "metadata": {},
   "source": [
    "## ML Notation\n",
    "\n",
    "We use standard notation in machine learning, and in fair machine learning speicfically. \n",
    "\n",
    "This is important because we want to be able to communicate,like we call the horizontal and vertical axes of a plot the `x` and `y` axes. \n",
    "\n",
    "The AIF 360 pacakge we are about to use and sklearn both use this notation. \n",
    "\n",
    "-  _target_ or _labels_, denoted by for one sample (row) $i$ $\\mathbf{y_i}$.\n",
    "- whole column of the target variable is $Y$\n",
    "- \"hat\" notation for predictions/ output of prediction algorithm $\\hat{y}_i$ and $\\hat{Y}$\n",
    "- \"protected attribute\" $a_i$ and $A$\n",
    "\n",
    "we use lowercase for one sample and uppercase for many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a88488b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'skmetrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m help(\u001b[43mskmetrics\u001b[49m\u001b[38;5;241m.\u001b[39maccuracy_score)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'skmetrics' is not defined"
     ]
    }
   ],
   "source": [
    "help(skmetrics.accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46477367",
   "metadata": {},
   "source": [
    "## Using AIF360\n",
    "\n",
    "The AIF360 package implements fairness metrics, some of which are derived from metrics we have seen and some others. [the documentation](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.ClassificationMetric.html#aif360.metrics.ClassificationMetric) has the full list in a summary table with English explanations and details with most equations.\n",
    "\n",
    "\n",
    "\n",
    "However, it has a few requirements:\n",
    "- its constructor takes two `BinaryLabelDataset` objects\n",
    "- these objects must be the same except for the label column\n",
    "- the constructor for `BinaryLabelDataset` only accepts all numerical DataFrames\n",
    "\n",
    "\n",
    "So, we have some preparation to do.  \n",
    "\n",
    "\n",
    "First, we'll make a numerical copy of the `compas_df` columns that we need. The only nonnumerical column that we need is race, wo we'll make a `dict` to replace that/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20243026",
   "metadata": {},
   "source": [
    "We need to used numerical values for the protected attribute. so lets make a mapping value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "573630a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m race_num_map \u001b[38;5;241m=\u001b[39m {r:i \u001b[38;5;28;01mfor\u001b[39;00m i,r, \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mcompas_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39mindex)}\n\u001b[1;32m      2\u001b[0m race_num_map\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_df' is not defined"
     ]
    }
   ],
   "source": [
    "race_num_map = {r:i for i,r, in enumerate(compas_df['race'].value_counts().index)}\n",
    "race_num_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dc237e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(race_num_map)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_df' is not defined"
     ]
    }
   ],
   "source": [
    "compas_df['race'].replace(race_num_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb7db9a",
   "metadata": {},
   "source": [
    "We will also only use a few of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dbef568",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m required_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo_year_recid\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_text_MedHigh\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m num_compas \u001b[38;5;241m=\u001b[39m \u001b[43mcompas_df\u001b[49m[required_cols]\u001b[38;5;241m.\u001b[39mreplace(race_num_map)\n\u001b[1;32m      3\u001b[0m num_compas\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_df' is not defined"
     ]
    }
   ],
   "source": [
    "required_cols = ['race','two_year_recid','score_text_MedHigh']\n",
    "num_compas = compas_df[required_cols].replace(race_num_map)\n",
    "num_compas.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084a621a",
   "metadata": {},
   "source": [
    "The scoring object requires that we have special data structures that wrap a DataFrame. \n",
    "\n",
    "We need one aif360 binary labeled dataset for the true values and one for the predictions. \n",
    "++\n",
    "\n",
    "Next we will make two versions, one with race & the ground truth and ht eother with race & the predictions. It's easiest to drop the column we don't want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a32dffc",
   "metadata": {},
   "source": [
    "The difference between the two datasets needs to be only the label column, so we drop the other variable from each small dataframe that we create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a2be0ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_compas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m num_compas_true \u001b[38;5;241m=\u001b[39m \u001b[43mnum_compas\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_text_MedHigh\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m num_compas_pred \u001b[38;5;241m=\u001b[39m num_compas\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo_year_recid\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_compas' is not defined"
     ]
    }
   ],
   "source": [
    "num_compas_true = num_compas.drop(columns=['score_text_MedHigh'])\n",
    "num_compas_pred = num_compas.drop(columns=['two_year_recid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abed6d6",
   "metadata": {},
   "source": [
    "Now we make the [`BinaryLabelDataset`](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.datasets.BinaryLabelDataset.html#aif360.datasets.BinaryLabelDataset) objects, this type comes from AIF360 too.  Basically, it is a DataFrame with extra attributes; some specific and some inherited from [`StructuredDataset`](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.datasets.StructuredDataset.html#aif360.datasets.StructuredDataset).\n",
    "\n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "remember, you can inspect *any* object using the `__dict__` attribute\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc956fc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BinaryLabelDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# here we want actual favorable outcome\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m broward_true \u001b[38;5;241m=\u001b[39m \u001b[43mBinaryLabelDataset\u001b[49m(favorable_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,unfavorable_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      3\u001b[0m                                   df \u001b[38;5;241m=\u001b[39m num_compas_true,\n\u001b[1;32m      4\u001b[0m                    label_names\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo_year_recid\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      5\u001b[0m                   protected_attribute_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      6\u001b[0m compas_predictions \u001b[38;5;241m=\u001b[39m BinaryLabelDataset(favorable_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,unfavorable_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      7\u001b[0m                                         df \u001b[38;5;241m=\u001b[39m num_compas_pred,\n\u001b[1;32m      8\u001b[0m                    label_names\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_text_MedHigh\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      9\u001b[0m                   protected_attribute_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BinaryLabelDataset' is not defined"
     ]
    }
   ],
   "source": [
    "# here we want actual favorable outcome\n",
    "broward_true = BinaryLabelDataset(favorable_label=0,unfavorable_label=1,\n",
    "                                  df = num_compas_true,\n",
    "                   label_names= ['two_year_recid'],\n",
    "                  protected_attribute_names=['race'])\n",
    "compas_predictions = BinaryLabelDataset(favorable_label=0,unfavorable_label=1,\n",
    "                                        df = num_compas_pred,\n",
    "                   label_names= ['score_text_MedHigh'],\n",
    "                  protected_attribute_names=['race'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf07151",
   "metadata": {},
   "source": [
    "This type also has an `ignore_fields` column for when comparisons are made, since the requirement is that only the *content* of the label column is different, but in our case also the label names are different, we have to tell it that that's okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90e6b1db",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# beacuse our columsn are named differently, we have to ignore that\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcompas_predictions\u001b[49m\u001b[38;5;241m.\u001b[39mignore_fields\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_names\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m broward_true\u001b[38;5;241m.\u001b[39mignore_fields\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_names\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "# beacuse our columsn are named differently, we have to ignore that\n",
    "compas_predictions.ignore_fields.add('label_names')\n",
    "broward_true.ignore_fields.add('label_names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4709e85e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fairmetrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m compas_fair_scorer \u001b[38;5;241m=\u001b[39m \u001b[43mfairmetrics\u001b[49m\u001b[38;5;241m.\u001b[39mClassificationMetric(broward_true,\n\u001b[1;32m      2\u001b[0m                                                       compas_predictions,\n\u001b[1;32m      3\u001b[0m                                  unprivileged_groups\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0\u001b[39m}],\n\u001b[1;32m      4\u001b[0m                                 privileged_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m1\u001b[39m}])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fairmetrics' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer = fairmetrics.ClassificationMetric(broward_true,\n",
    "                                                      compas_predictions,\n",
    "                                 unprivileged_groups=[{'race':0}],\n",
    "                                privileged_groups = [{'race':1}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f592f79d",
   "metadata": {},
   "source": [
    "Now we can use the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f244951a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39maccuracy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f321f9",
   "metadata": {},
   "source": [
    "By default, we get the overall accuracy.  This calculation matches what we got using sklearn. \n",
    "\n",
    "\n",
    "\n",
    "For the aif360 metrics, they have one parameter, `privleged` with a defautl value of `None` when it's none it computes th ewhole dataset.  When `True` it compues only the priveleged group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9ea110d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39maccuracy(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.accuracy(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3519a572",
   "metadata": {},
   "source": [
    "Here that is Caucasion people."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cee390",
   "metadata": {},
   "source": [
    "When `False` it's the unpriveleged group, here African American"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ffb538a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39maccuracy(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.accuracy(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b5e389",
   "metadata": {},
   "source": [
    "These again match what we calculated before, the advantaged group (White) for True and disadvantaged group (Black) for False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb97049b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39merror_rate_difference()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.error_rate_difference()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33abb9bc",
   "metadata": {},
   "source": [
    "the error rate alone does not tell the whole story because there are two types of errors. Plus there are even more ways we can think about if something is fair or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a62f176",
   "metadata": {},
   "source": [
    "### Disparate Impact \n",
    "\n",
    "One way we might want to be fair is if the same % of each group of people (Black, $A=0$ and White,$A=1$) get the favorable outcome (a low score). \n",
    "\n",
    "\n",
    "In Disparate Impact the ratio is of the positive outcome, independent of the predictor.  So this is the ratio of the % of Black people not rearrested to % of white people rearrested.\n",
    "\n",
    "\n",
    "\n",
    "$$D = \\frac{\\Pr(\\hat{Y} = 1|A=0)}{\\Pr(\\hat{Y} =1|A=1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1d6568",
   "metadata": {},
   "source": [
    "This is equivalent to saying that the score is unrelated to race."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c9d6b1",
   "metadata": {},
   "source": [
    "This type of fair is often the kind that most people think of intuitively. It is like dividing things equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7a41f60",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39mdisparate_impact()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.disparate_impact()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26dfb70",
   "metadata": {},
   "source": [
    "US court doctrine says that this quantity has to be above .8 for employment decisions.  Does COMPAS pass this criterion?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fb43bc",
   "metadata": {},
   "source": [
    "## Equalized Odds Fairness\n",
    "\n",
    "The journalists were concerned with the types of errors.  They accepted that it is not the creators of COMPAS fault that Black people get arrested at higher rates (though actual crime rates are equal; Black neighborhoods tend to be overpoliced). They wanted to consider what actually happened and then see how COMPAS did within each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb0554da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39mfalse_positive_rate(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.false_positive_rate(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3e378ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39mfalse_positive_rate(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.false_positive_rate(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339503c0",
   "metadata": {},
   "source": [
    "false positives are incorrectly got a low score.  \n",
    "\n",
    "This is different from how the problem was setup when we used sklearn because sklearn assumes tht 0 is the negative class and 1 is the \"positive\" class, but AIF360 lets us declre the favorable outcome(positive class) and unfavorable outcome (negative class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c2841b",
   "metadata": {},
   "source": [
    "White people were given a low score and then re-arrested almost twice as often as Black people."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dedb9c3",
   "metadata": {},
   "source": [
    "Black people were given a low score and then re-arrested only a little more than half as often as white people.  (White people were give an low score and rearrested almost twice as often)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c1cb4d",
   "metadata": {},
   "source": [
    "To make a single metric, we might take a ratio.  This is where the journalists [found bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0599255",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39mfalse_positive_rate_ratio()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.false_positive_rate_ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b92ea6",
   "metadata": {},
   "source": [
    "This metric would be fair with a value of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5203de07",
   "metadata": {},
   "source": [
    "got a high score and did not re-arrested  as a percentage of those who got a high score \n",
    "\n",
    "\n",
    "We can look at the other type of error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63822b85",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39mfalse_negative_rate(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.false_negative_rate(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e550121",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39mfalse_negative_rate(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.false_negative_rate(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b045df60",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39mfalse_negative_rate_ratio()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.false_negative_rate_ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1594de",
   "metadata": {},
   "source": [
    "Black people were given a high score and not rearrested almost twice as often as white people.\n",
    "\n",
    "So while the accuracy was similar (see error rate ratio) for Black and White people; the algorithm makes the opposite types of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4935a81e",
   "metadata": {},
   "source": [
    "### Average Odds Difference\n",
    "\n",
    "This is a combines the two errors we looked at separately into a single metric.  \n",
    "\n",
    "$$ \\tfrac{1}{2}\\left[(FPR_{A = \\text{unprivileged}} - FPR_{A = \\text{privileged}})\n",
    "   + (TPR_{A = \\text{unprivileged}} - TPR_{A = \\text{privileged}}))\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e6a99c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39maverage_odds_difference()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.average_odds_difference()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002ed739",
   "metadata": {},
   "source": [
    "**note** if time, discuss: \n",
    "- What should this look like if it is fair?\n",
    "- what could this metric hide?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa74f60c",
   "metadata": {},
   "source": [
    "After the journalists published the piece, the people who made COMPAS countered with a technical report, arguing that that the journalists had measured fairness incorrectly.\n",
    "\n",
    "The journalists two measures false positive rate and false negative rate use the true outcomes as the denominator.  \n",
    "\n",
    "## Sufficiency and Calibration\n",
    "\n",
    "The [COMPAS creators argued](https://www.equivant.com/response-to-propublica-demonstrating-accuracy-equity-and-predictive-parity/) that the model should be evaluated in terms of if a given score means the same thing across races; using the prediction as the denominator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5b531a",
   "metadata": {},
   "source": [
    "We can look at their preferred metrics too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ede229a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39mfalse_omission_rate(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.false_omission_rate(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d995d55c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39mfalse_omission_rate(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.false_omission_rate(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efcc1640",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39mfalse_omission_rate_ratio()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.false_omission_rate_ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b72ee792",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39mfalse_discovery_rate_ratio()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.false_discovery_rate_ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a42869e",
   "metadata": {},
   "source": [
    "On these two metrics, the ratio is closer to 1 and much less disparate.\n",
    "\n",
    "\n",
    "The creators thought it was important for the score to mean the same thing for every person assigned a score. The journalists thought it was more important for the algorithm to have the same impact of different groups of people.  \n",
    "Ideally, we would like the score to both mean the same thing for different people and to have the same impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95165db",
   "metadata": {},
   "source": [
    "Researchers established that these are mutually exclusive, provably.  We cannot have both, so it is very important to think about what the performance metrics mean and how your algorithm will be used in order to choose how to prepare a model.  We will train models starting next week, but knowing these goals in advance is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a253a1d3",
   "metadata": {},
   "source": [
    "Importantly, this is not a statistical, computational choice that data can answer for us. This is about *human* values (and to some extent the law; certain domains have legal protections that require a specific condition)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb0bf75",
   "metadata": {},
   "source": [
    "The Fair Machine Learning book's classification Chapter has a [section on relationships between criteria](https://fairmlbook.org/classification.html#relationships-between-criteria) with the proofs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed113ce1",
   "metadata": {},
   "source": [
    "To put it all together, we can make a plot. First we'll make a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3688b612",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ratios \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39mfalse_omission_rate_ratio(),\n\u001b[1;32m      2\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfalse ommission rate\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msufficiency\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreferred_by\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCOMPAS\u001b[39m\u001b[38;5;124m'\u001b[39m},\n\u001b[1;32m      5\u001b[0m          {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m:compas_fair_scorer\u001b[38;5;241m.\u001b[39mfalse_discovery_rate_ratio(),\n\u001b[1;32m      6\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfalse discovery rate\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msufficiency\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreferred_by\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCOMPAS\u001b[39m\u001b[38;5;124m'\u001b[39m},\n\u001b[1;32m      9\u001b[0m          {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m:compas_fair_scorer\u001b[38;5;241m.\u001b[39mfalse_positive_rate_ratio(),\n\u001b[1;32m     10\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfalse positive rate\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseparation\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreferred_by\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProPublica\u001b[39m\u001b[38;5;124m'\u001b[39m},\n\u001b[1;32m     13\u001b[0m          {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m:compas_fair_scorer\u001b[38;5;241m.\u001b[39mfalse_negative_rate_ratio(),\n\u001b[1;32m     14\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfalse negative rate\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     15\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseparation\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreferred_by\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProPublica\u001b[39m\u001b[38;5;124m'\u001b[39m}]\n\u001b[1;32m     17\u001b[0m ratio_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(ratios)\n\u001b[1;32m     18\u001b[0m ratio_df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "ratios = [{'score':compas_fair_scorer.false_omission_rate_ratio(),\n",
    "          'name': 'false ommission rate',\n",
    "          'group':'sufficiency',\n",
    "          'preferred_by':'COMPAS'},\n",
    "         {'score':compas_fair_scorer.false_discovery_rate_ratio(),\n",
    "          'name': 'false discovery rate',\n",
    "          'group':'sufficiency',\n",
    "          'preferred_by':'COMPAS'},\n",
    "         {'score':compas_fair_scorer.false_positive_rate_ratio(),\n",
    "          'name': 'false positive rate',\n",
    "          'group':'separation',\n",
    "          'preferred_by':'ProPublica'},\n",
    "         {'score':compas_fair_scorer.false_negative_rate_ratio(),\n",
    "          'name': 'false negative rate',\n",
    "          'group':'separation',\n",
    "          'preferred_by':'ProPublica'}]\n",
    "ratio_df = pd.DataFrame(ratios)\n",
    "ratio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58ddb445",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b871302d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241m.\u001b[39mcatplot(data\u001b[38;5;241m=\u001b[39mratio_df,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m,x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m,hue\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreferred_by\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      2\u001b[0m            kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m'\u001b[39m,aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      3\u001b[0m sns\u001b[38;5;241m.\u001b[39mlineplot(x \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m4\u001b[39m],y\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m],color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m'\u001b[39m,legend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "sns.catplot(data=ratio_df,y='score',x='name',hue='preferred_by',\n",
    "           kind='bar',aspect=2)\n",
    "sns.lineplot(x = [-1,4],y=[1,1],color='black',legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151d5388",
   "metadata": {},
   "source": [
    "These are all ratios, so 1 is fair. COMPAS does okay on the measures it was designed around and poorly on the ones the journalists preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0a94a2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39mfalse_omission_rate_difference()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.false_omission_rate_difference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11253e45",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39mfalse_discovery_rate_ratio()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.false_discovery_rate_ratio()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.14.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "source_map": [
   12,
   20,
   31,
   35,
   56,
   60,
   62,
   66,
   75,
   79,
   82,
   85,
   90,
   93,
   101,
   103,
   107,
   112,
   130,
   132,
   153,
   157,
   162,
   164,
   168,
   172,
   182,
   185,
   188,
   200,
   212,
   216,
   222,
   227,
   231,
   233,
   241,
   244,
   246,
   250,
   254,
   258,
   260,
   264,
   278,
   282,
   286,
   288,
   292,
   298,
   302,
   304,
   310,
   314,
   318,
   325,
   327,
   329,
   337,
   341,
   345,
   347,
   353,
   362,
   364,
   370,
   382,
   386,
   390,
   394,
   398,
   400,
   408,
   410,
   412,
   414,
   418,
   439,
   445,
   449,
   453,
   457
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}